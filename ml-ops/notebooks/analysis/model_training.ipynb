{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Pipeline\n",
    "\n",
    "This notebook implements the model training pipeline for two main tasks:\n",
    "1. Predicting network issues using network performance data\n",
    "2. Predicting customer churn using customer experience data\n",
    "\n",
    "We'll use various baseline models and evaluate their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "network_df = pd.read_csv(\"../../data/raw/sample_data/network_performance_sample.csv\")\n",
    "customer_df = pd.read_csv(\"../../data/raw/sample_data/customer_experience_sample.csv\")\n",
    "cdr_df = pd.read_csv(\"../../data/raw/sample_data/call_detail_records_sample.csv\")\n",
    "\n",
    "# Convert timestamps to datetime\n",
    "network_df[\"timestamp\"] = pd.to_datetime(network_df[\"timestamp\"])\n",
    "customer_df[\"timestamp\"] = pd.to_datetime(customer_df[\"timestamp\"])\n",
    "cdr_df[\"timestamp\"] = pd.to_datetime(cdr_df[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network Issue Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network issues based on performance metrics\n",
    "def identify_network_issues(df):\n",
    "    df = df.copy()\n",
    "    # Define thresholds for network issues using more lenient values\n",
    "    df['is_issue'] = (\n",
    "        (df['latency_ms'] > 50) |  # High latency (reduced from 100)\n",
    "        (df['packet_loss'] > 0.02) |  # High packet loss (reduced from 0.05)\n",
    "        (df['throughput_mbps'] < 50)  # Low throughput (increased from 5)\n",
    "    ).astype(int)\n",
    "    return df\n",
    "\n",
    "# Prepare network data for modeling\n",
    "network_issues_df = identify_network_issues(network_df)\n",
    "\n",
    "# Select features for network issue prediction\n",
    "#network_features = ['latency_ms', 'packet_loss', 'throughput_mbps', 'signal_strength_dbm']\n",
    "network_features = ['latency_ms', 'packet_loss', 'throughput_mbps', 'signal_strength']\n",
    "X_network = network_issues_df[network_features]\n",
    "y_network = network_issues_df['is_issue']\n",
    "\n",
    "# Split the data\n",
    "X_train_net, X_test_net, y_train_net, y_test_net = train_test_split(\n",
    "    X_network, y_network, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_net_scaled = scaler.fit_transform(X_train_net)\n",
    "X_test_net_scaled = scaler.transform(X_test_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(results).T\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Train and evaluate network issue models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m network_results = \u001b[43mtrain_and_evaluate_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_net_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_net_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_net\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNetwork Issue Prediction Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(network_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_and_evaluate_models\u001b[39m\u001b[34m(X_train, X_test, y_train, y_test)\u001b[39m\n\u001b[32m     10\u001b[39m results = {}\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m     16\u001b[39m     y_pred = model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitl/ml-ops/.venv/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gitl/ml-ops/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1301\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1299\u001b[39m classes_ = \u001b[38;5;28mself\u001b[39m.classes_\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_classes < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis solver needs samples of at least 2 classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1303\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the data, but the data contains only one\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % classes_[\u001b[32m0\u001b[39m]\n\u001b[32m   1305\u001b[39m     )\n\u001b[32m   1307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.classes_) == \u001b[32m2\u001b[39m:\n\u001b[32m   1308\u001b[39m     n_classes = \u001b[32m1\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)"
     ]
    }
   ],
   "source": [
    "# Train and evaluate network issue prediction models\n",
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'SVM': SVC(probability=True),\n",
    "        'XGBoost': XGBClassifier()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results[name] = {\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'F1 Score': f1_score(y_test, y_pred),\n",
    "            'ROC AUC': roc_auc_score(y_test, y_pred_proba)\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# Train and evaluate network issue models\n",
    "network_results = train_and_evaluate_models(\n",
    "    X_train_net_scaled, X_test_net_scaled, y_train_net, y_test_net\n",
    ")\n",
    "print(\"\\nNetwork Issue Prediction Results:\")\n",
    "print(network_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Customer Churn Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare customer data for modeling\n",
    "def prepare_customer_data(customer_df, network_df, cdr_df):\n",
    "    # Merge customer data with network performance\n",
    "    merged_df = pd.merge_asof(\n",
    "        customer_df.sort_values('timestamp'),\n",
    "        network_df.sort_values('timestamp'),\n",
    "        on='timestamp',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    \n",
    "    # Add CDR data\n",
    "    merged_df = pd.merge_asof(\n",
    "        merged_df.sort_values('timestamp'),\n",
    "        cdr_df.sort_values('timestamp'),\n",
    "        on='timestamp',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    \n",
    "    # Define churn based on customer satisfaction and usage patterns\n",
    "    merged_df['is_churn'] = (\n",
    "        (merged_df['customer_satisfaction_score'] < 3) |\n",
    "        (merged_df['data_usage_mb'] < merged_df['data_usage_mb'].mean() * 0.5)\n",
    "    ).astype(int)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Prepare the data\n",
    "customer_churn_df = prepare_customer_data(customer_df, network_df, cdr_df)\n",
    "\n",
    "# Select features for churn prediction\n",
    "churn_features = [\n",
    "    'customer_satisfaction_score', 'data_usage_mb', 'voice_minutes',\n",
    "    'sms_count', 'latency_ms', 'packet_loss', 'throughput_mbps'\n",
    "]\n",
    "X_churn = customer_churn_df[churn_features]\n",
    "y_churn = customer_churn_df['is_churn']\n",
    "\n",
    "# Split the data\n",
    "X_train_churn, X_test_churn, y_train_churn, y_test_churn = train_test_split(\n",
    "    X_churn, y_churn, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "X_train_churn_scaled = scaler.fit_transform(X_train_churn)\n",
    "X_test_churn_scaled = scaler.transform(X_test_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate churn prediction models\n",
    "churn_results = train_and_evaluate_models(\n",
    "    X_train_churn_scaled, X_test_churn_scaled, y_train_churn, y_test_churn\n",
    ")\n",
    "print(\"\\nCustomer Churn Prediction Results:\")\n",
    "print(churn_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "def plot_model_comparison(network_results, churn_results):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot network issue prediction results\n",
    "    network_results['F1 Score'].plot(kind='bar', ax=ax1)\n",
    "    ax1.set_title('Network Issue Prediction - F1 Scores')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot churn prediction results\n",
    "    churn_results['F1 Score'].plot(kind='bar', ax=ax2)\n",
    "    ax2.set_title('Customer Churn Prediction - F1 Scores')\n",
    "    ax2.set_ylabel('F1 Score')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the results\n",
    "plot_model_comparison(network_results, churn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
